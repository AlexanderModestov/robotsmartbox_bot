import logging
import tempfile
import os
import json
import time
from aiogram import Router, types, F
from aiogram.enums import ChatAction
from aiogram.fsm.context import FSMContext
from bot.services.elevenlabs import TextToSpeechService
from bot.config import Config
from aiogram.types import InlineKeyboardMarkup, InlineKeyboardButton, WebAppInfo, FSInputFile
import openai

# Create routers for question handling
question_router = Router()
query_router = Router()

def get_proper_title(content_type, original_title):
    """Get proper title from config files based on content type and original title"""
    try:
        # Map content_type to config file
        config_files = {
            'text': 'text_descriptions.json',
            'video': 'video_descriptions.json', 
            'podcast': 'podcast_descriptions.json',
            'audio': 'podcast_descriptions.json',  # Use podcast config for audio
            'url': 'url_descriptions.json'
        }
        
        config_file = config_files.get(content_type)
        if not config_file:
            return original_title
            
        # Load config file
        config_path = os.path.join(os.path.dirname(__file__), '..', 'configs', config_file)
        if not os.path.exists(config_path):
            return original_title
            
        with open(config_path, 'r', encoding='utf-8') as f:
            config_data = json.load(f)
            
        # Find matching entry in config
        content_key = 'texts' if content_type == 'text' else 'videos'
        if content_key in config_data and original_title in config_data[content_key]:
            return config_data[content_key][original_title]['name']
            
        return original_title
        
    except Exception as e:
        logging.warning(f"Error getting proper title: {e}")
        return original_title

# In-memory storage for pagination (in production, use Redis or database)
user_pagination_data = {}

async def transcribe_voice_cloud(message: types.Message) -> str:
    """Transcribe voice message using OpenAI Whisper API"""
    # Get the file
    file_id = message.voice.file_id if message.voice else message.audio.file_id
    file = await message.bot.get_file(file_id)
    
    temp_file = None
    try:
        with tempfile.NamedTemporaryFile(suffix='.ogg', delete=False) as temp_file:
            await message.bot.download_file(file.file_path, temp_file.name)
            
            # Use OpenAI Whisper API for transcription (v1.0+ syntax)
            client = openai.AsyncOpenAI()
            with open(temp_file.name, 'rb') as audio_file:
                transcript = await client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file,
                    language="ru"
                )
            
            return transcript.text.strip()
            
    finally:
        # Clean up temp file
        if temp_file and os.path.exists(temp_file.name):
            os.unlink(temp_file.name)


@question_router.message(F.text | F.voice | F.audio)
async def handle_user_question(message: types.Message, state: FSMContext, supabase_client):
    """Handle user questions with RAG pipeline"""
    # Extract text from message (text or voice)
    user_text = None
    
    if message.text:
        user_text = message.text
    elif message.voice or message.audio:
        # Show processing message for voice
        processing_voice_message = await message.answer("üé§ –†–∞—Å–ø–æ–∑–Ω–∞—é –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ...")
        
        try:
            user_text = await transcribe_voice_cloud(message)
            await processing_voice_message.delete()
            
            if not user_text or user_text.strip() == "":
                await message.answer("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑ –∏–ª–∏ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ.")
                return
                
        except Exception as e:
            logging.error(f"Error transcribing voice: {e}")
            await processing_voice_message.edit_text("–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.")
            return
    else:
        await message.answer("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ—Ç–ø—Ä–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –∏–ª–∏ –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –≤–∞—à–∏–º –∑–∞–ø—Ä–æ—Å–æ–º.")
        return
    
    # Show processing message
    processing_message = await message.answer("ü§î –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –≤–∞—à –≤–æ–ø—Ä–æ—Å...")

        # Send typing action
    await message.bot.send_chat_action(
        chat_id=message.from_user.id, 
        action=ChatAction.TYPING
    )
    
    try:
        # Get user from database
        user = await supabase_client.get_user_by_telegram_id(message.from_user.id)
        if not user:
            await processing_message.edit_text("–û—à–∏–±–∫–∞: –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—É /start")
            return
        
        # Create OpenAI client and call API with n8n automation expert prompt
        client = openai.AsyncOpenAI()
        
        n8n_prompt = """–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ n8n. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ–± –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∑–∞–¥–∞—á –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤.

–í–ê–ñ–ù–û: –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –ù–ï —Å–≤—è–∑–∞–Ω —Å –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–µ–π –∑–∞–¥–∞—á, –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∏–ª–∏ —Ä–∞–±–æ—á–∏—Ö –ø–æ—Ç–æ–∫–æ–≤, –æ—Ç–≤–µ—á–∞–π –¢–û–ß–ù–û: "–Ø –Ω–µ –∑–Ω–∞—é –æ—Ç–≤–µ—Ç –Ω–∞ –í–∞—à –≤–æ–ø—Ä–æ—Å. –Ø –º–æ–≥—É –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å —Ç–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç—ã –ø–æ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏."

–î–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –æ–± –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å –∫—Ä–∞—Ç–∫–∏–π –æ—Ç–≤–µ—Ç —Å —ç—Ç–∏–º–∏ 3 —Ä–∞–∑–¥–µ–ª–∞–º–∏:

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
ü§ñ –†–µ—à–µ–Ω–∏–µ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ n8n (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
–û–ø–∏—à–∏ —Ç–æ—á–Ω–æ, –∫–∞–∫ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å —ç—Ç—É –∑–∞–¥–∞—á—É, –∏—Å–ø–æ–ª—å–∑—É—è —É–∑–ª—ã –∏ —Ä–∞–±–æ—á–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã n8n.

‚úÖ –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ (3 –ø—É–Ω–∫—Ç–∞)
‚Ä¢ –≠–∫–æ–Ω–æ–º–∏—è –≤—Ä–µ–º–µ–Ω–∏: [–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —á–∞—Å—ã/–Ω–µ–¥–µ–ª—é —Å—ç–∫–æ–Ω–æ–º–ª–µ–Ω—ã]
‚Ä¢ –°–Ω–∏–∂–µ–Ω–∏–µ –æ—à–∏–±–æ–∫: [% —É–ª—É—á—à–µ–Ω–∏–µ]
‚Ä¢ –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å: [–ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —Ä–æ—Å—Ç–∞]

üí∞ –≠–∫–æ–Ω–æ–º–∏—è —Å—Ä–µ–¥—Å—Ç–≤ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
–†–∞—Å—Å—á–∏—Ç–∞–π –ø—Ä–∏–º–µ—Ä–Ω—É—é –º–µ—Å—è—á–Ω—É—é —ç–∫–æ–Ω–æ–º–∏—é –≤ –¥–æ–ª–ª–∞—Ä–∞—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—á–∞—Å–æ–≤—ã—Ö —Å—Ç–∞–≤–æ–∫ –∏ —Å—ç–∫–æ–Ω–æ–º–ª–µ–Ω–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏.

–ü—Ä–∞–≤–∏–ª–∞:
- –ù–µ —É–∫–∞–∑—ã–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è —É–∑–ª–æ–≤ n8n, –∏—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –æ–ø–∏—Å–∞–Ω–∏–µ
- –ò—Å–ø–æ–ª—å–∑—É–π —á–∏—Å–ª–∞ –∏ –ø—Ä–æ—Ü–µ–Ω—Ç—ã
- –û–±—â–∏–π –æ—Ç–≤–µ—Ç –Ω–µ –±–æ–ª–µ–µ 150 —Å–ª–æ–≤
- –°–æ—Å—Ä–µ–¥–æ—Ç–æ—á—å—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ —Å–∞–º–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏
- –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–π —Å—Ç–æ–∏–º–æ—Å—Ç—å —Ç—Ä—É–¥–∞ $25/—á–∞—Å –¥–ª—è —Ä–∞—Å—á–µ—Ç–æ–≤"""

        response = await client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": n8n_prompt},
                {"role": "user", "content": f"–ó–∞–¥–∞—á–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏: {user_text}"}
            ],
            max_tokens=500,
            temperature=0.7
        )
        
        response_text = response.choices[0].message.content
        
        # No sources needed for this automation response
        keyboard = None
        
        # Check if user prefers audio responses
        if user.isAudio:
            try:
                # Generate audio using ElevenLabs
                logging.info(f"üéß Generating audio response for user {message.from_user.id}")
                tts_service = TextToSpeechService()
                
                # Generate audio file
                audio_path = tts_service.text_to_speech(
                    text=response_text,
                    quality_preset="conversational",  # Good for bot responses
                    output_filename=f"response_{message.from_user.id}_{int(time.time())}.mp3"
                )
                
                # Send audio file
                audio_file = FSInputFile(audio_path)
                await processing_message.delete()  # Delete processing message
                
                if keyboard:
                    # Send audio with sources buttons
                    await message.answer_audio(
                        audio=audio_file,
                        reply_markup=keyboard,
                        caption="üéß –ê—É–¥–∏–æ–æ—Ç–≤–µ—Ç"
                    )
                else:
                    # Send audio without buttons
                    await message.answer_audio(
                        audio=audio_file,
                        caption="üéß –ê—É–¥–∏–æ–æ—Ç–≤–µ—Ç"
                    )
                
                # Clean up audio file
                try:
                    os.unlink(audio_path)
                except OSError:
                    pass  # File cleanup failed, but not critical
                
                logging.info(f"‚úÖ Successfully sent audio response to user {message.from_user.id}")
                return
                
            except Exception as audio_error:
                logging.error(f"‚ö†Ô∏è Audio generation failed, falling back to text: {audio_error}")
                # Fall back to text response if audio generation fails
        
        # Send text response (either user prefers text or audio generation failed)
        logging.info(f"üì§ RAG Step 5: Response Formatting - Sending final response (length: {len(response_text)} chars)")
        try:
            await processing_message.edit_text(response_text, reply_markup=keyboard, parse_mode="Markdown")
            logging.info(f"‚úÖ RAG Step 5: Response Formatting - Successfully sent response with Markdown")
        except Exception as markdown_error:
            # Fallback: send without markdown if parsing fails
            logging.warning(f"‚ö†Ô∏è RAG Step 5: Response Formatting - Markdown parsing failed, sending as plain text: {markdown_error}")
            await processing_message.edit_text(response_text, reply_markup=keyboard)
            logging.info(f"‚úÖ RAG Step 5: Response Formatting - Successfully sent response as plain text")
        
    except Exception as e:
        logging.error(f"‚ùå RAG Pipeline: Fatal error processing question for user {message.from_user.id}: {e}")
        await processing_message.edit_text(
            "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑ –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É."
        )


